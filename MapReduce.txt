When a file is larger than the block size it is divided into multiple blocks each of block size and when a mapreduce job is submitted on top of the particular file two tasks are performed 1. map task 2. reduce task
The map task will be executed not on top of the block but on top of the input split.
input split is such that when a file is splitted into multiple blocks the end of the particular line may get splitted into some other block so while executing the map task it needs to fetch that data also to execute the map task. so the ideal scenario for input split should be the block size as there will not be any network congestion involved in fetching the input split data from the other block.
There is no guarantee that a map task will get executed on that particular machine itself, because the machine may be already occupied with some of the map tasks or busy in some executions.so this map task should be executed on any of the machines which has sufficient resources to execute the map task.so the data need to be transferred into the machine which has sufficient resources so this can be carried out in 3 phases using of rack awareness::
1.data local data locality
data present on the same node where the computation exists
2.rack local data locality
data present in the different node than where the computation exists but both of these nodes are present in the same rack 
3.off rack local data locality
data present in the different node than where the computation exists but both of these nodes are present in the different racks

The output of map task is stored temporarily in the node where the computation has completed.so incase while executing the reduce task if the result for map task doesn't exists it will re run the map task for that particular input split.but the output of the reduce tasks is stored in the hdfs.
The map task is performed in 3 phases::
1.sort
2.copy
3.merge

so,while executing the map phase if it showing the status as (0-100%) then it is executing these 3 phases.
0-33.3 for sort
33.4-66.3 for copy
66.4-100 for merge

Question::why the sort operation is performed before the copy operation??
Question Detail:: suppose,for a NCDC Weather dataset
  IP1-->M1 (AUS,NZ)
  IP2-->M2 (AUS,Swiss)
  
  
  doubts::
  -------
  1.what will happen on the merge phase?
  
links::
http://ercoppa.github.io/HadoopInternals/
